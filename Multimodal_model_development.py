# -*- coding: utf-8 -*-
"""Finetuned_multimodal_chatbot_development.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15V8paklRmyqh7w87t3ESkaxLGTyvbI6b
"""

# installing dependencies
!pip install -q accelerate transformers safetensors bitsandbytes datasets peft torchaudio gradio torchvision --quiet
!pip install git+https://github.com/huggingface/diffusers

# Importing dependencies
import os
os.environ["WANDB_DISABLED"] = "true"
import pandas as pd
from PIL import Image
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from transformers import BlipProcessor, BlipForConditionalGeneration, CLIPTokenizer, CLIPTextModel, CLIPFeatureExtractor, WhisperProcessor, WhisperForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline, GenerationConfig
import torch
from torch.optim import AdamW
from tqdm import tqdm
from diffusers import StableDiffusionPipeline, DDPMScheduler, UNet2DConditionModel, AutoencoderKL
import json
from datasets import load_dataset, Audio, Dataset
from dataclasses import dataclass
from typing import Dict, List, Union
import torch.nn as nn

# Parse Captions and Prepare Dataset
# Define paths
image_dir = "/content/drive/MyDrive/Colab_Notebooks/multimodal_data/images"
caption_path = "/content/drive/MyDrive/Colab_Notebooks/multimodal_data/captions.txt"

# Read the CSV-like caption file
df = pd.read_csv(caption_path, header=0, names=["image", "caption"])

# Optional: Reduce to a smaller subset for Colab limits
df = df.sample(n=min(len(df), 500), random_state=42)

# (Optional) Remove whitespace from image paths if needed
df['image'] = df['image'].str.strip()

# Dataset Class
class FlickrDataset(Dataset):
    def __init__(self, dataframe, image_dir, processor):
        self.data = dataframe
        self.image_dir = image_dir
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        image = Image.open(os.path.join(self.image_dir, row['image'])).convert('RGB')
        encoding = self.processor(images=image, text=row['caption'], padding="max_length", return_tensors="pt", max_length=32, truncation=True)
        encoding = {k: v.squeeze() for k, v in encoding.items()}
        encoding['labels'] = encoding['input_ids']
        return encoding

# Training Loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base").to(device)

dataset = FlickrDataset(df, image_dir, processor)
loader = DataLoader(dataset, batch_size=4, shuffle=True)

optimizer = AdamW(model.parameters(), lr=5e-5)

model.train()
for epoch in range(1):
    loop = tqdm(loader, leave=True)
    for batch in loop:
        inputs = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**inputs)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        loop.set_description(f"Epoch {epoch}")
        loop.set_postfix(loss=loss.item())

# Inference: Image → Caption
model.eval()
sample_image_path = os.path.join(image_dir, df.iloc[0]['image'])
image = Image.open(sample_image_path).convert('RGB')
inputs = processor(images=image, return_tensors="pt").to(device)
out = model.generate(**inputs)
print("Prediction:", processor.decode(out[0], skip_special_tokens=True))

# Commented out IPython magic to ensure Python compatibility.
# dependencies import and installation
# %cd /content/drive/MyDrive/Colab_Notebooks/
# !git clone https://github.com/huggingface/diffusers
# %cd diffusers
# !pip install -e .

# Create the metadata.jsonl file
excel_file = '/content/drive/MyDrive/Colab_Notebooks/multimodal_data/captions.xlsx'
images_dir = '/content/drive/MyDrive/Colab_Notebooks/multimodal_data/images'
output_jsonl = os.path.join(images_dir, 'metadata.jsonl')

# Load Excel file
df = pd.read_excel(excel_file)

# Ensure column names are correct
assert 'image' in df.columns and 'caption' in df.columns, "Excel must have 'image' and 'caption' columns."

# Write JSONL file
with open(output_jsonl, 'w') as jsonlfile:
    for _, row in df.iterrows():
        file_name = os.path.basename(row['image'])
        caption = str(row['caption']).strip()
        json_line = {"file_name": file_name, "text": caption}
        jsonlfile.write(json.dumps(json_line) + '\n')

import shutil

drive_path = "/content/drive/MyDrive/Colab_Notebooks/multimodal_data/images"
local_path = "/content/multimodal_train_data"

shutil.copytree(drive_path, local_path, dirs_exist_ok=True)

# Dataset retrieve
df = pd.read_json("/content/multimodal_train_data/metadata.jsonl", lines=True)
print(df.iloc[0])

# Data modelling
class CustomImageTextDataset(Dataset):
    def __init__(self, jsonl_path, images_dir):
        self.images_dir = images_dir
        with open(jsonl_path, 'r') as f:
            self.data = [json.loads(line) for line in f]
        self.transform = transforms.Compose([
            transforms.Resize((512, 512)),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5])
        ])

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]
        image_path = os.path.join(self.images_dir, item["file_name"])
        image = Image.open(image_path).convert("RGB")
        image = self.transform(image)
        return {"pixel_values": image, "text": item["text"]}


train_dataset = CustomImageTextDataset(
    jsonl_path="/content/multimodal_train_data/metadata.jsonl",
    images_dir="/content/multimodal_train_data/"
)

# Load pretrained components from Stable Diffusion
vae = AutoencoderKL.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="vae")
unet = UNet2DConditionModel.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="unet")
noise_scheduler = DDPMScheduler.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="scheduler")
tokenizer = CLIPTokenizer.from_pretrained("openai/clip-vit-large-patch14")
text_encoder = CLIPTextModel.from_pretrained("openai/clip-vit-large-patch14")
# safety_checker = StableDiffusionSafetyChecker.from_pretrained("CompVis/stable-diffusion-v1-4", subfolder="safety_checker")
feature_extractor = CLIPFeatureExtractor.from_pretrained("openai/clip-vit-large-patch14")

device = "cuda" if torch.cuda.is_available() else "cpu"

train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)
for batch in train_dataloader:
    inputs = tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=tokenizer.model_max_length,
        return_tensors="pt"
    ).input_ids
    pixel_values = batch["pixel_values"]

    # You can now use `inputs` and `pixel_values` for training/fine-tuning
    break  # remove this when training loop is implemented


# Rebuild the pipeline from your fine-tuned components
pipeline = StableDiffusionPipeline(
    vae=vae,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    unet=unet,
    scheduler=noise_scheduler,
    safety_checker=None,
    feature_extractor=feature_extractor
).to(device)

pipeline.save_pretrained("/content/multimodal_train_data/sd-text2img")

# image generation from caption
pipe = StableDiffusionPipeline.from_pretrained(
    "/content/multimodal_train_data/sd-text2img",
    torch_dtype=torch.float16
).to("cuda")

prompt = "a group of young men standing in front of a sign"
image = pipe(prompt).images[0]
image.save("/content/multimodal_train_data/custom_generated.png")
display(image)

# 1. INSTALL DEPENDENCIES
!pip install -q git+https://github.com/openai/whisper.git     # Whisper repo + model
!pip install -q librosa jiwer

# Replace these with your actual Google Drive (or local) paths:
AUDIO_DIR = "/content/drive/MyDrive/Colab_Notebooks/multimodal_data/audio"
METADATA_CSV = "/content/drive/MyDrive/Colab_Notebooks/multimodal_data/metadata.csv"

# ─────────────────────────────────────────────────────────────────────────────
# 3. LOAD AND INSPECT METADATA
# ─────────────────────────────────────────────────────────────────────────────
# Read metadata.csv to confirm contents
df = pd.read_csv(METADATA_CSV)
df = df[["file_name", "text"]]  # Only keep necessary columns
print("Rows of metadata:")
print(df.head())

# Ensure required columns exist
assert "file_name" in df.columns and "text" in df.columns, "EXCEL must have 'file_name' and 'text' columns."

# ─────────────────────────────────────────────────────────────────────────────
# 4. LOAD DATA WITH `datasets.Dataset`
# ─────────────────────────────────────────────────────────────────────────────
# We load via the EXCEL and then cast the `audio` column to an Audio feature
# so that when we .map(..., batched=False), the actual waveform is loaded.

# 4.1 Create a Dataset from the CSV
# Convert to Hugging Face Dataset
asr_dataset = Dataset.from_pandas(df)


# 4.2 Add a column "audio" by mapping file_name → actual audio waveform from disk
def load_audio_sample(batch):
    # Construct full path
    path = os.path.join(AUDIO_DIR, batch["file_name"])
    # `datasets.Audio` will load waveform and sampling_rate fields
    batch["audio"] = {"path": path}
    return batch

asr_dataset = asr_dataset.map(load_audio_sample)

# 4.3 Cast the "audio" column to the Audio feature type (loads waveform on the fly)
asr_dataset = asr_dataset.cast_column("audio", Audio(sampling_rate=16_000))

# Inspect a single example
print("\nA single example:")
print(asr_dataset[0])

# ─────────────────────────────────────────────────────────────────────────────
# 5. PREPROCESSING: FEATURE EXTRACTION & TOKENIZATION
# ─────────────────────────────────────────────────────────────────────────────
# 5.1 Load Whisper processor & model (base or small—choose depending on your GPU)
model_name = "openai/whisper-small"  # you can also use "openai/whisper-base" or "openai/whisper-tiny"
processor = WhisperProcessor.from_pretrained(model_name)
model = WhisperForConditionalGeneration.from_pretrained(model_name)

# Move model to GPU if available
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# 5.2 Preprocessing function
#   - Loads audio["array"] + audio["sampling_rate"]
#   - Generates input_features (log-Mel spectrogram)
#   - Tokenizes the target text into labels

def preprocess_function(batch):
    # 1. Load and resample audio
    audio_array = batch["audio"]["array"]
    sampling_rate = batch["audio"]["sampling_rate"]
    # WhisperProcessor will handle resampling to 16k if needed:
    inputs = processor(audio_array, sampling_rate=sampling_rate, return_tensors="pt")

    # 2. Tokenize text
    labels = processor.tokenizer(batch["text"]).input_ids


    # 3. Append fields
    batch["input_features"] = inputs.input_features[0]
    batch["labels"] = labels
    return batch

# 5.3 Apply preprocessing (non-batched to avoid memory spikes)
asr_dataset = asr_dataset.map(preprocess_function, remove_columns=["file_name", "audio", "text"])

# 5.4 Final dataset columns: ["input_features", "labels"]
print("\nColumns after preprocessing:", asr_dataset.column_names)
print("Sample preprocessed features/labels shapes:")
print("input_features length:", len(asr_dataset[0]["input_features"]))
print("input_features shape:", torch.tensor(asr_dataset[0]["input_features"]).shape)
# If labels is an int or list
print("labels:", asr_dataset[0]["labels"])

print(torch.tensor(asr_dataset[0]["input_features"]).shape, asr_dataset[0]["labels"])

# ─────────────────────────────────────────────────────────────────────────────
# 6. DATA COLLATOR
# ─────────────────────────────────────────────────────────────────────────────

@dataclass
class DataCollatorForWhisper:
    processor: WhisperProcessor
    padding: Union[bool, str] = True

    def __call__(self, features: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:
        # 1. Pad audio features (log-Mel spectrogram)
        input_features = [f["input_features"] for f in features]
        batch_inputs = self.processor.feature_extractor.pad(
            {"input_features": input_features},
            return_tensors="pt"
        )

        # 2. Pad/shift labels
        labels = [f["labels"] for f in features]
        labels_batch = self.processor.tokenizer.pad(
            {"input_ids": labels},
            padding=self.padding,
            return_tensors="pt"
        ).input_ids

        # Replace padding token id's of the labels by -100 so it's ignored by the loss
        labels_batch[labels_batch == processor.tokenizer.pad_token_id] = -100

        batch_inputs["labels"] = labels_batch
        return batch_inputs

data_collator = DataCollatorForWhisper(processor=processor)

# ─────────────────────────────────────────────────────────────────────────────
# 7. TRAINING SETUP (HUGGINGFACE `Trainer`)
# ─────────────────────────────────────────────────────────────────────────────
# 7.1 Training arguments
training_args = Seq2SeqTrainingArguments(
    output_dir="/content/drive/MyDrive/Colab_Notebooks/multimodal_data/whisper_finetuned_asr",
    per_device_train_batch_size=4,        # Adjust based on your GPU (try 4 or 8)
    gradient_accumulation_steps=2,        # Simulate larger batch
    learning_rate=3e-5,
    num_train_epochs=3,
    fp16=torch.cuda.is_available(),       # Mixed precision if possible
    logging_steps=50,
    save_steps=500,
    save_total_limit=2,
    predict_with_generate=False,          # We're not generating text→text
    remove_unused_columns=False,          # We’re specifying our own collator
)

# 7.2 Initialize Trainer
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=asr_dataset,
    data_collator=data_collator,
    tokenizer=processor.tokenizer,        # For proper handling of label/pad tokens
)

# ─────────────────────────────────────────────────────────────────────────────
# 8. RUN FINE-TUNING
# ─────────────────────────────────────────────────────────────────────────────

trainer.train()

# ─────────────────────────────────────────────────────────────────────────────
# 9. SAVE FINE-TUNED MODEL
# ─────────────────────────────────────────────────────────────────────────────

trainer.save_model("/content/drive/MyDrive/Colab_Notebooks/multimodal_data/whisper_finetuned_asr")
# Save the processor (includes feature extractor + tokenizer)
processor.save_pretrained("/content/drive/MyDrive/Colab_Notebooks/multimodal_data/whisper_finetuned_asr")

# inference audio to text
# Load model and processor
model_path = "/content/drive/MyDrive/Colab_Notebooks/multimodal_data/whisper_finetuned_asr"
processor = WhisperProcessor.from_pretrained(model_path)
model = WhisperForConditionalGeneration.from_pretrained(model_path)

# Move to device
device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

# ⚠️ REMOVE forced_decoder_ids from config (IMPORTANT!)
model.config.forced_decoder_ids = None
model.generation_config.forced_decoder_ids = None  # <- required in some versions

# Load audio
audio_path = "/content/drive/MyDrive/Colab_Notebooks/multimodal_data/audio/cen2-mgah-b.wav"
waveform, sample_rate = torchaudio.load(audio_path)

# Resample to 16kHz if needed
if sample_rate != 16000:
    waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)

# Convert to mono if stereo
if waveform.shape[0] > 1:
    waveform = waveform.mean(dim=0, keepdim=True)

# Convert audio to numpy array
input_audio = waveform.squeeze().numpy()

# Process input
inputs = processor(input_audio, sampling_rate=16000, return_tensors="pt")
input_features = inputs.input_features.to(device)

# ✅ Generate without triggering forced_decoder_ids
with torch.no_grad():
    predicted_ids = model.generate(input_features=input_features)

# Decode the prediction
transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
print("Transcription:", transcription)

# voice to image inference

# load audio to text  model
model_path = "/content/drive/MyDrive/Colab_Notebooks/multimodal_data/whisper_finetuned_asr"
# load text to image model
pipe = StableDiffusionPipeline.from_pretrained(
    "/content/multimodal_train_data/sd-text2img",
    torch_dtype=torch.float16
).to("cuda")

def voice_to_text(audio_path):
    processor = WhisperProcessor.from_pretrained(model_path)
    model = WhisperForConditionalGeneration.from_pretrained(model_path)

    # Move to device
    device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    # ⚠️ REMOVE forced_decoder_ids from config (IMPORTANT!)
    model.config.forced_decoder_ids = None
    model.generation_config.forced_decoder_ids = None  # <- required in some versions

    waveform, sample_rate = torchaudio.load(audio_path)
    # Resample to 16kHz if needed
    if sample_rate != 16000:
        waveform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)(waveform)

    # Convert to mono if stereo
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    # Convert audio to numpy array
    input_audio = waveform.squeeze().numpy()

    # Process input
    inputs = processor(input_audio, sampling_rate=16000, return_tensors="pt")
    input_features = inputs.input_features.to(device)

    # ✅ Generate without triggering forced_decoder_ids
    with torch.no_grad():
        predicted_ids = model.generate(input_features=input_features)

    # Decode the prediction
    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription

def text_to_image(prompt, out_path="/content/drive/MyDrive/Colab_Notebooks/multimodal_data/image.png"):
    image = pipe(prompt).images[0]
    image.save(out_path)
    return image

def voice_to_image(audio_path):
    text = voice_to_text(audio_path)
    return text_to_image(text)

voice_to_image("/content/drive/MyDrive/Colab_Notebooks/multimodal_data/audio/cen2-mgah-b.wav")
